# Research Questions & Hypotheses

## Central Question

**Do judges unconsciously award higher scores for impressive performances when those performances immediately follow multiple wipeouts or poor performances?** (Contrast/Relief Effect)

---

## Specific Hypotheses

### H1: Contrast Bias from Recent Wipeouts
Performers who land clean runs score HIGHER if preceded by wipeouts, compared to seeing other clean runs.

**Example**: Chase JOSEY wiped out in R1 (11.75), then scored 70.25 in R2 — was this partly a "relief" bonus?

### H2: Cumulative Wipeout Effect
The MORE wipeouts immediately preceding a clean run, the HIGHER the score (controlling for trick difficulty).

**Example**: 
- Chaeun LEE: Wipeout → Wipeout → Score 87.50
- Ziyang WANG: Wipeout → Wipeout → Score 76.00

### H3: Position/Ordering Bias Within Rounds
Later performers (who've watched more performances) benefit from or are disadvantaged by pattern effects.

---

## What We're Testing

### ✅ Can Analyze Now
1. **Wipeout context** for each clean run
2. **Recovery score patterns** (after own/others' wipeouts)
3. **Performer trajectories** across rounds (W-W-H, W-H patterns)
4. **Within-round ordering effects** (position vs score)
5. **Judge consensus** patterns (which judges excluded most frequently)

### ❌ Cannot Analyze (Need More Data)
- **Trick difficulty**: Need official difficulty ratings per run
- **Judge component scores**: amplitude, difficulty, execution, variety, progression
- **Environmental factors**: Weather, snow conditions, course changes
- **Timing**: Duration between runs, judge rest periods
- **Judge composition**: Whether same 6 judges scored all rounds

---

## Why This Matters

This analysis could reveal:
1. **Systematic judging bias** in Olympic halfpipe (ordering effects)
2. **Psychological factors** in how judges evaluate performances
3. **Fairness implications** — are early competitors disadvantaged?
4. **Judge training needs** — awareness of contrast bias

Similar patterns found in: Olympic gymnastics, figure skating, wine competitions.

---

## Future Analysis Directions

Now that we have individual judge scores and trick data:

### A) Judge Outlier Analysis
Which judges are most frequently excluded (high/low)?
- Track which judges are outliers most often
- Test: Do certain judges have systematic biases?

### B) Middle-4 Judge Consensus Bias
Do 3+ judges simultaneously boost scores for same tricks?
- Filter to runs with identical/near-identical trick sequences
- Test: Do they score same execution higher after wipeouts?

### C) Individual Judge Context Sensitivity
Which judges show strongest relief bias patterns?
- For each judge, correlate their scores with preceding wipeout count

### D) Context Effect Magnitude by Round
Does relief bias change between R1, R2, R3?
- R1: Most wipeouts, judges "fresh"
- R3: No wipeouts, only elite performers

### E) Within-Score Variance Analysis
When 4 middle judges disagree, what causes the spread?
- Does variance increase after wipeouts (judges uncertain)?
- Does variance predict relief bonus magnitude?

---

## Data Source

**Official**: https://www.olympics.com/en/milano-cortina-2026/results/sbd/je/m/hp----------------/fnl-/--------/result

- Event: Milano-Cortina 2026 Men's Snowboard Halfpipe Final
- Date: February 13, 2026
- Competitors: 12
- Rounds: 3 (round-robin, same order each round)
- Judges: 6 per run, high/low excluded, middle 4 averaged
- Data: Individual judge scores, trick sequences, final rankings

---

## Related Research

This analysis connects to:
- **Contrast bias** in judging and evaluation
- **Anchoring effects** in subjective scoring
- **Recency bias** and performance context
- **Fairness in judged sports** (gymnastics, diving, figure skating)
